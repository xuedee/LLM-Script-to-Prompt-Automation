# -*- coding: utf-8 -*-
"""script-to-storyboard-prompt-generator.ipynb
created by DX
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/130z9Z1Rhwey8Qsa-SWr02nx_mYvdgR7e
"""

# Model caching directory (local or cloud path when applicable)
cache_dir = "qwen_models_cache"

# --- Library Imports ---
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr
import re
import requests  # Added for stable translation API access

# --- Model Loading ---
def load_model(model_name: str, cache_dir: str):
    """Loads the tokenizer and the model from HuggingFace."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            # Use dtype instead of torch_dtype (removes deprecation warning)
            dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨ç¼“å­˜ç›®å½•/Model loaded successfully, using cache directory:", cache_dir)
        return tokenizer, model
    except Exception as e:
        print("æ¨¡å‹åŠ è½½å¤±è´¥/Model loading failed:", e)
        return None, None

# Load Model (Step 6)
MODEL_NAME = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer, model = load_model(MODEL_NAME, cache_dir)

# --- Prompt Construction ---
def build_prompt(scene_text):
    """Constructs the prompt for the Qwen model. Focuses only on Chinese description."""
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå½±åˆ†é•œå¸ˆï¼Œæ“…é•¿å°†å‰§æœ¬åœºæ™¯æ‰©å±•ä¸ºè¯¦ç»†ç”»é¢æè¿°ã€‚è¯·**åªè¾“å‡º**è¯¦ç»†çš„**ä¸­æ–‡ç”»é¢æè¿°**ï¼Œ**ä¸è¦**è¾“å‡ºä»»ä½•è‹±æ–‡ç¿»è¯‘æˆ–å…¶å®ƒé¢å¤–çš„æ–‡å­—ã€‚"
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"è¯·å¤„ç†ä»¥ä¸‹åœºæ™¯ï¼š\n{scene_text}"}
    ]

    full_prompt = ""
    for msg in messages:
        full_prompt += f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>\n"
    full_prompt += "<|im_start|>assistant\n"

    return full_prompt


# --- Output Parsing (Robust Logic) ---
def extract_outputs(output_text):
    """
    Robustly parses the LLM output.
    Returns: chinese_md, english_md_placeholder, debug_note, translate_required (bool)
    """
    # Attempt to match the desired Chinese title (Less likely to be found due to small LLM behavior)
    chinese_match = re.search(
        r"ã€ä¸­æ–‡ç”»é¢æè¿°ã€‘\s*[:ï¼š]\s*(.*?)\s*(?:ã€English Prompt for AI Image Generationã€‘|$)",
        output_text, re.DOTALL
    )

    # Attempt to match the English title
    english_match = re.search(
        r"ã€English Prompt for AI Image Generationã€‘\s*[:ï¼š]\s*(.*)",
        output_text, re.DOTALL
    )

    if chinese_match:
        # Scenario 1: Found the Chinese title
        chinese_md = chinese_match.group(1).strip()

        if english_match:
            # Scenario 1a: LLM provided both titles (Ideal, but rare)
            english_md = english_match.group(1).strip()
            debug_note = "æˆåŠŸè§£æä¸­æ–‡å’Œè‹±æ–‡æ ‡é¢˜/Successfully parsed Chinese and English titles."
            translate_required = False
        else:
            # Scenario 1b: Found Chinese title, but no English title
            english_md = "å¾…è‡ªåŠ¨ç¿»è¯‘/Pending automatic translation..."  # Placeholder for translation
            debug_note = "æ‰¾åˆ°ä¸­æ–‡æ ‡é¢˜ä½†è‹±æ–‡Promptæ ‡é¢˜ç¼ºå¤±ã€‚æ ‡è®°ä¸ºå¾…è‡ªåŠ¨ç¿»è¯‘/Found Chinese title but missing English Prompt title. Marked for auto-translation."
            translate_required = True

    else:
        # Scenario 2: Fallback (Most common case: LLM ignores formatting and outputs only raw Chinese text)
        chinese_md = output_text.strip()
        english_md = "å¾…è‡ªåŠ¨ç¿»è¯‘/Pending automatic translation..."  # Placeholder for translation
        debug_note = "å…³é”®æ ‡é¢˜è§£æå¤±è´¥ï¼Œæ¨¡å‹æœªéµå¾ªæ ¼å¼.å°†æ‰€æœ‰è¾“å‡ºè§†ä¸ºä¸­æ–‡æè¿°ã€‚æ ‡è®°ä¸ºå¾…è‡ªåŠ¨ç¿»è¯‘ã€‚/Key title parsing failed, model did not follow format. All output treated as Chinese description. Marked for auto-translation."
        translate_required = True

    # Returns the required flag for Step 9
    return chinese_md, english_md, debug_note, translate_required


# --- Main Generation Function ---
def generate_scene(scene_text, temperature, top_p, max_tokens):
    """Main logic to generate description, parse output, and translate if necessary."""
    debug_log = f"ğŸ¬ è¾“å…¥åœºæ™¯æ–‡æœ¬/Input scene text:\n{scene_text}\n\n"

    # Internal function for stable translation using the Google Translate unofficial API via requests
    def translate_chinese_sync(text_to_translate):
        """Translates Chinese text to English using the stable requests method."""
        if not text_to_translate or text_to_translate.startswith("âš ï¸"):
            return "âš ï¸ ç¿»è¯‘å¤±è´¥ï¼šè¾“å…¥å†…å®¹ä¸ºç©ºæˆ–å·²æ˜¯é”™è¯¯æç¤ºã€‚/Translation failed: Input content is empty or an error message."

        # Limit text length for stability
        MAX_LEN = 4500
        if len(text_to_translate) > MAX_LEN:
            text_to_translate = text_to_translate[:MAX_LEN] + "..."

        # Google Translate API unofficial URL
        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            'client': 'gtx',
            'sl': 'zh-CN',  # Source Language: Chinese
            'tl': 'en',     # Target Language: English
            'dt': 't',
            'q': text_to_translate # Query text
        }

        try:
            # Send GET request with timeout
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status() # Check for HTTP errors

            # Parse JSON response
            data = response.json()

            # Extract translated text (chunks are usually in data[0])
            translated_chunks = [chunk[0] for chunk in data[0]]

            # Concatenate the chunks
            return "".join(translated_chunks)

        except requests.exceptions.Timeout:
            return "âŒ è‡ªåŠ¨ç¿»è¯‘å¤±è´¥ (Timeout): ç¿»è¯‘è¯·æ±‚è¶…æ—¶ã€‚è¯·ç¨åå†è¯•æˆ–æ‰‹åŠ¨ç¿»è¯‘/Automatic translation failed (Timeout): Request timed out. Please try again or translate manually."
        except Exception as e:
            return f"âŒè‡ªåŠ¨ç¿»è¯‘å¤±è´¥ (API Error): {e}ã€‚è¯·æ‰‹åŠ¨ç¿»è¯‘ä¸Šæ–¹çš„ä¸­æ–‡æè¿°/ Automatic translation failed (API Error): {e}. Please translate the Chinese description manually."


    try:
        # 1. LLM Generation
        prompt_text = build_prompt(scene_text)
        inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            eos_token_id=tokenizer.eos_token_id
        )

        output_text = tokenizer.decode(
            output_ids[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        debug_log += f"ğŸ“¤ Raw Model Output:\n{output_text}\n"

        # 2. Parse LLM Output (Receives 4 return values)
        chinese_md, english_md_placeholder, debug_note, translate_required = extract_outputs(output_text)

        # 3. Automatic Translation (if required)
        if translate_required:
            translated_text = translate_chinese_sync(chinese_md)
            english_md = translated_text
            # Log success if translation was not an error message
            if not english_md.startswith("âŒ"):
                debug_log += f"âœ… Successfully called requests API for auto-translation.\n"
        else:
            english_md = english_md_placeholder # Use English output provided by the model

        debug_log += debug_note

        return chinese_md, english_md, debug_log

    except Exception as e:
        debug_log += f"\nâŒ Generation or translation failed: {str(e)}"
        return "Error", "Error", debug_log
# End of generate_scene function


# --- Gradio Interface Launch ---
with gr.Blocks() as demo:
    gr.Markdown("ğŸ¬ **Script Scene â†’ Storyboard Description & AI Image Prompt**")

    scene_input = gr.Textbox(lines=5, label="ğŸ­ Input Script Scene")
    generate_btn = gr.Button("ğŸš€ Generate Description and English Prompt")

    with gr.Row():
        # Fine-tuned generation parameters
        temperature = gr.Slider(0.1, 1.5, value=0.95, step=0.1, label="Temperature")
        top_p = gr.Slider(0.1, 1.0, value=0.95, step=0.05, label="Top-p")
        max_tokens = gr.Slider(64, 1024, value=512, step=64, label="Max Output Tokens")

    # Output textboxes with fixed height (lines=10)
    output_cn = gr.Textbox(label="ğŸ–‹ï¸ ä¸­æ–‡ç”»é¢æè¿°/Chinese Scene Description", lines=10)
    output_en = gr.Textbox(label="ğŸŒ Prompt for AI Image Generation (English)", lines=10)
    debug_out = gr.Textbox(label="ğŸ Debug Log (æ¨¡å‹åŸå§‹è¾“å‡ºä¸é”™è¯¯ä¿¡æ¯/Raw Model Output and Errors)", lines=10)

    generate_btn.click(
        fn=generate_scene,
        inputs=[scene_input, temperature, top_p, max_tokens],
        outputs=[output_cn, output_en, debug_out]
    )

demo.launch()
