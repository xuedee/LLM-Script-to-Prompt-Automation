# -*- coding: utf-8 -*-
"""script-to-storyboard-prompt-generator.ipynb
created by DX
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/130z9Z1Rhwey8Qsa-SWr02nx_mYvdgR7e
"""

# Model caching directory (local or cloud path when applicable)
cache_dir = "qwen_models_cache"

# --- Library Imports ---
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr
import re
import requests  # Added for stable translation API access

# --- Model Loading ---
def load_model(model_name: str, cache_dir: str):
    """Loads the tokenizer and the model from HuggingFace."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            # Use dtype instead of torch_dtype (removes deprecation warning)
            dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("模型加载成功，使用缓存目录/Model loaded successfully, using cache directory:", cache_dir)
        return tokenizer, model
    except Exception as e:
        print("模型加载失败/Model loading failed:", e)
        return None, None

# Load Model (Step 6)
MODEL_NAME = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer, model = load_model(MODEL_NAME, cache_dir)

# --- Prompt Construction ---
def build_prompt(scene_text):
    """Constructs the prompt for the Qwen model. Focuses only on Chinese description."""
    system_prompt = (
        "你是一个电影分镜师，擅长将剧本场景扩展为详细画面描述。请**只输出**详细的**中文画面描述**，**不要**输出任何英文翻译或其它额外的文字。"
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"请处理以下场景：\n{scene_text}"}
    ]

    full_prompt = ""
    for msg in messages:
        full_prompt += f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>\n"
    full_prompt += "<|im_start|>assistant\n"

    return full_prompt


# --- Output Parsing (Robust Logic) ---
def extract_outputs(output_text):
    """
    Robustly parses the LLM output.
    Returns: chinese_md, english_md_placeholder, debug_note, translate_required (bool)
    """
    # Attempt to match the desired Chinese title (Less likely to be found due to small LLM behavior)
    chinese_match = re.search(
        r"【中文画面描述】\s*[:：]\s*(.*?)\s*(?:【English Prompt for AI Image Generation】|$)",
        output_text, re.DOTALL
    )

    # Attempt to match the English title
    english_match = re.search(
        r"【English Prompt for AI Image Generation】\s*[:：]\s*(.*)",
        output_text, re.DOTALL
    )

    if chinese_match:
        # Scenario 1: Found the Chinese title
        chinese_md = chinese_match.group(1).strip()

        if english_match:
            # Scenario 1a: LLM provided both titles (Ideal, but rare)
            english_md = english_match.group(1).strip()
            debug_note = "成功解析中文和英文标题/Successfully parsed Chinese and English titles."
            translate_required = False
        else:
            # Scenario 1b: Found Chinese title, but no English title
            english_md = "待自动翻译/Pending automatic translation..."  # Placeholder for translation
            debug_note = "找到中文标题但英文Prompt标题缺失。标记为待自动翻译/Found Chinese title but missing English Prompt title. Marked for auto-translation."
            translate_required = True

    else:
        # Scenario 2: Fallback (Most common case: LLM ignores formatting and outputs only raw Chinese text)
        chinese_md = output_text.strip()
        english_md = "待自动翻译/Pending automatic translation..."  # Placeholder for translation
        debug_note = "关键标题解析失败，模型未遵循格式.将所有输出视为中文描述。标记为待自动翻译。/Key title parsing failed, model did not follow format. All output treated as Chinese description. Marked for auto-translation."
        translate_required = True

    # Returns the required flag for Step 9
    return chinese_md, english_md, debug_note, translate_required


# --- Main Generation Function ---
def generate_scene(scene_text, temperature, top_p, max_tokens):
    """Main logic to generate description, parse output, and translate if necessary."""
    debug_log = f"🎬 输入场景文本/Input scene text:\n{scene_text}\n\n"

    # Internal function for stable translation using the Google Translate unofficial API via requests
    def translate_chinese_sync(text_to_translate):
        """Translates Chinese text to English using the stable requests method."""
        if not text_to_translate or text_to_translate.startswith("⚠️"):
            return "⚠️ 翻译失败：输入内容为空或已是错误提示。/Translation failed: Input content is empty or an error message."

        # Limit text length for stability
        MAX_LEN = 4500
        if len(text_to_translate) > MAX_LEN:
            text_to_translate = text_to_translate[:MAX_LEN] + "..."

        # Google Translate API unofficial URL
        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            'client': 'gtx',
            'sl': 'zh-CN',  # Source Language: Chinese
            'tl': 'en',     # Target Language: English
            'dt': 't',
            'q': text_to_translate # Query text
        }

        try:
            # Send GET request with timeout
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status() # Check for HTTP errors

            # Parse JSON response
            data = response.json()

            # Extract translated text (chunks are usually in data[0])
            translated_chunks = [chunk[0] for chunk in data[0]]

            # Concatenate the chunks
            return "".join(translated_chunks)

        except requests.exceptions.Timeout:
            return "❌ 自动翻译失败 (Timeout): 翻译请求超时。请稍后再试或手动翻译/Automatic translation failed (Timeout): Request timed out. Please try again or translate manually."
        except Exception as e:
            return f"❌自动翻译失败 (API Error): {e}。请手动翻译上方的中文描述/ Automatic translation failed (API Error): {e}. Please translate the Chinese description manually."


    try:
        # 1. LLM Generation
        prompt_text = build_prompt(scene_text)
        inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            eos_token_id=tokenizer.eos_token_id
        )

        output_text = tokenizer.decode(
            output_ids[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        debug_log += f"📤 Raw Model Output:\n{output_text}\n"

        # 2. Parse LLM Output (Receives 4 return values)
        chinese_md, english_md_placeholder, debug_note, translate_required = extract_outputs(output_text)

        # 3. Automatic Translation (if required)
        if translate_required:
            translated_text = translate_chinese_sync(chinese_md)
            english_md = translated_text
            # Log success if translation was not an error message
            if not english_md.startswith("❌"):
                debug_log += f"✅ Successfully called requests API for auto-translation.\n"
        else:
            english_md = english_md_placeholder # Use English output provided by the model

        debug_log += debug_note

        return chinese_md, english_md, debug_log

    except Exception as e:
        debug_log += f"\n❌ Generation or translation failed: {str(e)}"
        return "Error", "Error", debug_log
# End of generate_scene function


# --- Gradio Interface Launch ---
with gr.Blocks() as demo:
    gr.Markdown("🎬 **Script Scene → Storyboard Description & AI Image Prompt**")

    scene_input = gr.Textbox(lines=5, label="🎭 Input Script Scene")
    generate_btn = gr.Button("🚀 Generate Description and English Prompt")

    with gr.Row():
        # Fine-tuned generation parameters
        temperature = gr.Slider(0.1, 1.5, value=0.95, step=0.1, label="Temperature")
        top_p = gr.Slider(0.1, 1.0, value=0.95, step=0.05, label="Top-p")
        max_tokens = gr.Slider(64, 1024, value=512, step=64, label="Max Output Tokens")

    # Output textboxes with fixed height (lines=10)
    output_cn = gr.Textbox(label="🖋️ 中文画面描述/Chinese Scene Description", lines=10)
    output_en = gr.Textbox(label="🌍 Prompt for AI Image Generation (English)", lines=10)
    debug_out = gr.Textbox(label="🐞 Debug Log (模型原始输出与错误信息/Raw Model Output and Errors)", lines=10)

    generate_btn.click(
        fn=generate_scene,
        inputs=[scene_input, temperature, top_p, max_tokens],
        outputs=[output_cn, output_en, debug_out]
    )

demo.launch()
